{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "COMP1804_Lab_7_CNN_CIFAR10.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97A12EGJM5n"
      },
      "source": [
        "# Convolutional Neural Network with Keras (Part 1) \n",
        "\n",
        "**What is Keras?** Keras is a wrapper that allows you to implement Deep Neural Network without getting into intrinsic details of the Network. It can use *Tensorflow* or *Theano* as backend. \n",
        "\n",
        "\n",
        "In this lab you will build a **CNN** (comprised of convolutional layers, pooling layers and fully connected ones) for image classification (classify whether an image contains an airplane or automobile or bird or cat or deer or dog or frog or horse or ship or truck)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GmbVXIzJM5p"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.638667Z",
          "start_time": "2020-06-10T16:54:29.084221Z"
        },
        "id": "sZ_qSs-rJM5q"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSwAI53VJM5v"
      },
      "source": [
        "# Importing Dataset\n",
        "\n",
        "Here we are loading CIFAR10 Dataset which is preloaded in tensorflow. <br>\n",
        "\n",
        "Calling the `load_data` function on this object returns splitted train and test data in form of (features, target)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.82622Z",
          "start_time": "2020-06-10T16:54:31.640652Z"
        },
        "id": "sN__Hw1sJM5v"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY2BQittJM5z"
      },
      "source": [
        "# Overview of Dataset\n",
        "\n",
        "The CIFAR10 dataset contains 60,000 (32 x 32 pixel) color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.<br>\n",
        ">The shape (50000, 32, 32, 3) represents **50000** images each of dimension **32 x 32 x 3**.<br>\n",
        "The shape **(50000, )** represents (50000, 1) shape i.e. 50000 labels, each for one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.832844Z",
          "start_time": "2020-06-10T16:54:31.828686Z"
        },
        "id": "oF4wHayFJM50"
      },
      "source": [
        "print(f'Shape of the training data: {train_images.shape}')\n",
        "print(f'Shape of the training target: {train_labels.shape}')\n",
        "print(f'Shape of the test data: {test_images.shape}')\n",
        "print(f'Shape of the test target: {test_labels.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.839396Z",
          "start_time": "2020-06-10T16:54:31.835735Z"
        },
        "id": "qfKCqRKWJM53"
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.048741Z",
          "start_time": "2020-06-10T16:54:31.84145Z"
        },
        "id": "k8rWdBCGJM56"
      },
      "source": [
        "# To verify that the dataset looks correct, let's plot the first 16 images from the training set and display the class name below each image.\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(32,32))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m83ZM21PJM58"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "Normalizing i.e. scaling the pixels to 0-1 from 0-255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.275587Z",
          "start_time": "2020-06-10T16:54:32.05088Z"
        },
        "id": "i34Q3nNYJM59"
      },
      "source": [
        "# Normalizing\n",
        "train_images, test_images = tf.cast(train_images,tf.float32) / 255.0, tf.cast(test_images,tf.float32) / 255.0\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-V7_qjzOnco"
      },
      "source": [
        "Or alternatively standardizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKaDBK5FcHL_"
      },
      "source": [
        "#Standardizing \n",
        "import pdb\n",
        "def standardize(image_data):\n",
        "        image_data = image_data.astype(float)\n",
        "        mean = np.mean(image_data, axis=0)\n",
        "        image_data -= mean\n",
        "        std = np.std(image_data, axis=0)\n",
        "        image_data /= std\n",
        "        return image_data, mean, std\n",
        "\n",
        "train_images, mean, std =   standardize(train_images)\n",
        "\n",
        "def standardize_test(image_data, mean, std):\n",
        "        image_data = image_data.astype(float)\n",
        "        image_data -= mean\n",
        "        image_data /= std\n",
        "        return image_data\n",
        "\n",
        "test_images =   standardize_test(test_images, mean, std)\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16BVy3LTOr5J"
      },
      "source": [
        "# convert to grayscale\n",
        "# train_images, test_images = tf.image.rgb_to_grayscale(train_images), tf.image.rgb_to_grayscale(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9nZEGrvJM6A"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "There are two types of models in Tensorflow:\n",
        " - **Sequential**\n",
        " - **Graphical**\n",
        "\n",
        "## Models\n",
        "`tf.keras.model.Sequential()` \n",
        "lets you create a linear stack of layers providing a Sequential netural network.<br>\n",
        "`tf.model()`\n",
        "allows you to create arbitarary graph of layers as long as there is no cycle.\n",
        "\n",
        "## Flatten Layer\n",
        "`tf.keras.layers.Flatten()` flattens the input.<br>\n",
        "For input of `(batch_size, height, width, depth)` the output converts to `(batch_size, height * width * depth)`\n",
        "\n",
        "\n",
        "\n",
        "## Dense Layer\n",
        "`tf.keras.layers.Dense()` Normal dense layer (= fully connected layer): each node/neuron in this layer is connected to each node in the input layer. <br>\n",
        ">The two arguments passes below in dense layer are *units* and *activation* (activation function).<br>\n",
        "* **units** corresponds to the number of nodes in the layer<br>\n",
        "* **activation** is an element-wise activation function.\n",
        "    * **'relu'**: This activation function converts every negative value to 0 and positive remains the same\n",
        "    * **'softmax'**: This function takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. The elements of the output vector are in range (0, 1) and sum to 1.\n",
        "    * **'sigmoid'**: Applies the sigmoid activation function. For small values (<-5), sigmoid returns a value close to zero, and for large values (>5) the result of the function gets close to 1.\n",
        "    * **'tanh'**: Applies the hyperbolic tangient function. The range of the tanh function is from (-1 to 1).\n",
        "    * **None**: It means there is no activation function. In other words this layer produces linear output.\n",
        "\n",
        "## Convolution Layer\n",
        "`tf.keras.layers.Conv2D()` Convolution layer takes the following argument\n",
        "> * **filters** Number of different types of convolutions used. Initially they are set to some predefined convolution and slowly trained to find better features in the image.\n",
        "* **kernel_size** An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "* **strides** An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Default value is (1,1)\n",
        "* **padding** one of \"valid\" or \"same\" (case-insensitive): \n",
        "  * **'VALID'**: it means no padding and it assumes that all the dimensions are valid so that the input image gets fully covered by a filter and the stride specified by you.\n",
        "  * **'SAME'**: it applies padding to the input image so that the input image gets fully covered by the filter and specified stride. It is called SAME because, for stride 1 , the output will be the same as the input. \n",
        "* **activation** activation function  (typical options were explained before). Default value is: None\n",
        "* **use_bias** Boolean, whether the layer uses a bias vector. Default value is: True\n",
        "* **input_shape** Size of each input to the convolution.\n",
        "\n",
        "## Pooling\n",
        "`tf.keras.layers.MaxPooling2D()` Max Pooling layer to reduce the size of the input. This layer takes the following arguments:\n",
        "> * **pool_size** Dimension of pooling kernel. Default value is (2, 2)\n",
        "* **strides** Integer, tuple of 2 integers, or None. Strides values. Specifies how far the pooling window moves for each pooling step. Default value is: None\n",
        "* **padding** One of \"valid\" or \"same\" (case-insensitive). \"valid\" adds no zero padding. \"same\" adds padding such that if the stride is 1, the output shape is the same as input shape. Default value is \"valid\".<br>\n",
        "`tf.keras.layers.AveragePooling2D()` Average Pooling layer to reduce the size of the input.\n",
        "\n",
        "## Regularazation:\n",
        "\n",
        "# Dropout\n",
        "`tf.keras.layers.Dropout()` Applies Dropout to the input.\n",
        "The Dropout layer randomly sets input units to 0 with a frequency of 'rate' at each step during training time, which helps prevent overfitting. \n",
        "It takes the following argument:\n",
        "> * **rate**: Float between 0 and 1. Fraction of the input units to drop.\n",
        "\n",
        "## Compiling model\n",
        "`model.compile()` Sets up the optimiser, loss and metrics configuration.\n",
        "> * **optimizer**: updates the parameter of the Neural Network.\n",
        "* **loss**: Measures the error in our model.\n",
        "* **metrics**: Used to judge the model. The difference between metrics and loss is that metrics in not used to evaluate the model while training, whereas loss evaluates the model error while training and helps optimizer reduce the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzmEthPJM6B"
      },
      "source": [
        "# Creating the Model-Architecture\n",
        "\n",
        "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size; color_channels refer to (R,G,B). \n",
        "In this example, we will configure our CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. We can do this by passing the argument input_shape to our first layer. </br>\n",
        "**Note** We only pass the argument input_shape to the first layer of the architecture. We do not have to pass it to any other layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:07:37.288653Z",
          "start_time": "2020-06-10T17:07:37.229297Z"
        },
        "id": "2Z43BeGFJM6B"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), padding = 'SAME', activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding = 'SAME', activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(layers.Conv2D(64, (3, 3), padding = 'SAME', activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# as metric we choose the accuracy: the total number of correct predictions made\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Y6hbfXJM6E"
      },
      "source": [
        "## Model details\n",
        "\n",
        "Let's look at details of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:24:41.162476Z",
          "start_time": "2020-06-10T17:24:41.157906Z"
        },
        "id": "WccMxDoOJM6E"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjKoESaaJM6H"
      },
      "source": [
        "## Training\n",
        "\n",
        "```model.fit``` trains the model.\n",
        "> * **train_images**: Training data/features\n",
        "* **train_labels**: Target\n",
        "* **epochs**: Number of times the entire dataset is fed in the model. In other words, one forward pass and one backward pass of all the training examples.\n",
        "* **batch size**: Number of images that will be fed to the network at each iteration for optimizing. In other words, the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:03.569655Z",
          "start_time": "2020-06-10T16:54:32.3542Z"
        },
        "id": "1zXaQqKqJM6H"
      },
      "source": [
        "# Training\n",
        "history = model.fit(train_images, train_labels, epochs=20, batch_size=256, \n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQZmUztwUzOU"
      },
      "source": [
        "predictions = model.predict(test_images)\n",
        "predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I2kHDrdUz3T"
      },
      "source": [
        "predictions_index = np.argmax(predictions, axis=1) # Convert one-hot to index; remember indexing starts from 0; index takes integers values in [0,9]\n",
        "predictions_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbZ7Y8uJM6J"
      },
      "source": [
        "## Visualize prediction\n",
        "\n",
        "Now let's visualize the prediction using the model you just trained. \n",
        "First we get the predictions with the model from the test data.\n",
        "Then we print out 15 images from the test data set, and set the titles with the prediction (and the ground truth label).\n",
        "If the prediction matches the true label, the title will be green; otherwise it's displayed in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xavvSORHJM6K"
      },
      "source": [
        "import pdb\n",
        "y_hat = model.predict(test_images)\n",
        "\n",
        "# Plot a random sample of 15 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for i, index in enumerate(np.random.choice(test_images.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(test_images[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = test_labels[index][0]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(class_names[predict_index], \n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8BZCkYBJM6P"
      },
      "source": [
        "# Visualising feature maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:05.523866Z",
          "start_time": "2020-06-10T16:59:03.572384Z"
        },
        "id": "aKprwqLxJM6P"
      },
      "source": [
        "from numpy import expand_dims\n",
        "from keras.models import Model\n",
        "\n",
        "# redefine model to output right after the first hidden layer # you can change here the selected layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "model1.summary()\n",
        "# load the image with the required shape # you can change here the selected training/test image \n",
        "img = train_images[0]\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# get feature map for first hidden layer; the feature map has dimensions: \n",
        "feature_maps = model1.predict(img)\n",
        "# plot all 64 maps in an 8x8 squares  # if you change the number of layers in the model you may need to change here the squares: their product should be equal to the number of feature maps in the layer that you have selected\n",
        "square1 = 8\n",
        "square2 = 8\n",
        "ix = 1\n",
        "\n",
        "plt.imshow(img[0])\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for _ in range(square1):\n",
        "\tfor _ in range(square2):\n",
        "\t\t# specify subplot and turn of axis\n",
        "\t\tax = figure.add_subplot(square1, square2, ix, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tax.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUhCVV33XKya"
      },
      "source": [
        "## Try/Check the following (in random order)\n",
        "\n",
        "1) Standardization vs Normalization </br>\n",
        "2) Graysclale vs RGB </br>\n",
        "3) Changing the number of feature maps in each conv layer </br>\n",
        "4) Adding more layers to the network (conv, pooling, dense layers) </br>\n",
        "5) Uncommenting the commented code that adds dropout regularization: experiment with putting dropout after various layers of the CNN and also changing its rate parameter </br>\n",
        "6) Using 'SAME' instead of 'VALID' padding  </br>\n",
        "7) Using different activation functions </br>\n",
        "8) Using different optimizers </br>\n",
        "9) Using different learning rates </br>\n",
        "10) Using different batch size (what's the difference in the performance and in the training time) </br>\n",
        "11) Train for more epochs </br>"
      ]
    }
  ]
}