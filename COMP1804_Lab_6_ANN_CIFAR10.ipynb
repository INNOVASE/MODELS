{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COMP1804_Lab_6_ANN_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97A12EGJM5n"
      },
      "source": [
        "# Artificial Neural Network with Keras \n",
        "\n",
        "**What is Keras?** Keras is a wrapper that allows you to implement (Deep) Neural Networks without getting into intrinsic details of the Network. It can use *Tensorflow* or *Theano* as backend. \n",
        "\n",
        "\n",
        "In this lab you will build a **ANN** (Perceptron/MLP) for image classification (classify whether an image contains an airplane or automobile or bird or cat or deer or dog or frog or horse or ship or truck)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GmbVXIzJM5p"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.638667Z",
          "start_time": "2020-06-10T16:54:29.084221Z"
        },
        "id": "sZ_qSs-rJM5q"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSwAI53VJM5v"
      },
      "source": [
        "# Importing Dataset\n",
        "\n",
        "Here we are loading the cifar10 Dataset which is preloaded in tensorflow. <br>\n",
        "\n",
        "Calling the `load_data` function on this object returns splitted train and test data in form of (features, target)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.82622Z",
          "start_time": "2020-06-10T16:54:31.640652Z"
        },
        "id": "sN__Hw1sJM5v"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY2BQittJM5z"
      },
      "source": [
        "# Overview of Dataset\n",
        "\n",
        "The CIFAR10 dataset contains 60,000 (32 x 32 pixel) color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.<br>\n",
        ">The shape (50000, 32, 32, 3) represents **50000** images each of dimension **32 x 32 x 3**.<br>\n",
        "The shape **(50000, )** represents (50000, 1) shape i.e. 50000 labels, each for one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.832844Z",
          "start_time": "2020-06-10T16:54:31.828686Z"
        },
        "id": "oF4wHayFJM50"
      },
      "source": [
        "print(f'Shape of the training data: {train_images.shape}')\n",
        "print(f'Shape of the training target: {train_labels.shape}')\n",
        "print(f'Shape of the test data: {test_images.shape}')\n",
        "print(f'Shape of the test target: {test_labels.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.839396Z",
          "start_time": "2020-06-10T16:54:31.835735Z"
        },
        "id": "qfKCqRKWJM53"
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.048741Z",
          "start_time": "2020-06-10T16:54:31.84145Z"
        },
        "id": "k8rWdBCGJM56"
      },
      "source": [
        "# To verify that the dataset looks correct, let's plot the first 16 images from the training set and display the class name below each image.\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(32,32))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m83ZM21PJM58"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "Normalizing i.e. scaling the pixels to 0-1 from 0-255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.275587Z",
          "start_time": "2020-06-10T16:54:32.05088Z"
        },
        "id": "i34Q3nNYJM59"
      },
      "source": [
        "# Normalizing\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN6S9bjmXr-8"
      },
      "source": [
        "Or alternatively standardizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8UFXLp32TS3"
      },
      "source": [
        "#Standardizing \n",
        "\n",
        "def standardize(image_data):\n",
        "        image_data = image_data.astype(float)\n",
        "        image_data -= np.mean(image_data, axis=0)\n",
        "        image_data /= np.std(image_data, axis=0)\n",
        "        return image_data\n",
        "\n",
        "train_images, test_images =   standardize(train_images), standardize(test_images)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-im5NNT8V3L"
      },
      "source": [
        "# convert to grayscale\n",
        "#train_images, test_images = tf.image.rgb_to_grayscale(train_images), tf.image.rgb_to_grayscale(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9nZEGrvJM6A"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "There are two types of models in Tensorflow:\n",
        " - **Sequential**\n",
        " - **Graphical**\n",
        "\n",
        "## Models\n",
        "`tf.keras.model.Sequential()` \n",
        "lets you create a linear stack of layers providing a Sequential netural network.<br>\n",
        "`tf.model()`\n",
        "allows you to create arbitarary graph of layers as long as there is no cycle.\n",
        "\n",
        "## Flatten Layer\n",
        "`tf.keras.layers.Flatten()` flattens the input.<br>\n",
        "For input of `(batch_size, height, width, depth)` the output converts to `(batch_size, height * width * depth)`\n",
        "\n",
        "## Dense Layer\n",
        "`tf.keras.layers.Dense()` Normal dense layer (= fully connected layer): each node/neuron in this layer is connected to each node in the input layer. <br>\n",
        ">The two arguments passes below in dense layer are *units* and *activation* (activation function).<br>\n",
        "* **units** corresponds to the number of nodes in the layer<br>\n",
        "* **activation** is an element-wise activation function.\n",
        "    * **'relu'**: This activation function converts every negative value to 0 and positive remains the same\n",
        "    * **'softmax'**: This function takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. The elements of the output vector are in range (0, 1) and sum to 1.\n",
        "    * **'sigmoid'**: Applies the sigmoid activation function. For small values (<-5), sigmoid returns a value close to zero, and for large values (>5) the result of the function gets close to 1.\n",
        "    * **'tanh'**: Applies the hyperbolic tangient function. The range of the tanh function is from (-1 to 1).\n",
        "    * **None**: It means there is no activation function. In other words this layer produces linear output.\n",
        "\n",
        "\n",
        "## Compiling model\n",
        "`model.compile()` Sets up the optimiser, loss and metrics configuration.\n",
        "> **optimizer**: updates the parameters (weights/biases) of the Neural Network. Some typical options:\n",
        "* **'adam'**: Optimizer that implements the Adam algorithm. It is computationally efficient/fast, has little memory requirement and is well suited for problems that are large in terms of data/parameters. \n",
        "* **'SGD'**: Gradient descent (with momentum) optimizer.\n",
        "Note: For both optimizers you need to define the learning rate\n",
        "\n",
        "> **loss**: Measures the error in our model. Some typical options:\n",
        "* **tf.keras.losses.CategoricalCrossentropy()**: Use this crossentropy loss function when there are two or more label classes (multi-class classification). We expect labels to be provided in a one_hot representation. \n",
        "* **tf.keras.losses.SparseCategoricalCrossentropy()**: Use this crossentropy loss function when there are two or more label classes (multi-class classification). We expect labels to be provided as integers.\n",
        "* **tf.keras.losses.BinaryCrossentropy()**: Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). It can also be used in multi-label classification cases.\n",
        "* **tf.keras.losses.MAE()**:  Computes the mean absolute error between labels and predictions. It is mostly used in regression tasks.\n",
        "* **tf.keras.losses.MSE()**: Computes the mean squared error between labels and predictions. It is mostly used in regression tasks.\n",
        "\n",
        "\n",
        "> **metrics**: Used to judge the model. The difference between metrics and loss is that metrics in not used to evaluate the model while training, whereas loss evaluates the model error while training and helps optimizer reduce the error. All options can be found in: \n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzmEthPJM6B"
      },
      "source": [
        "# Creating the Model-Architecture\n",
        "\n",
        "As input, a P/MLP takes tensors of shape (image_height x image_width x color_channels), ignoring the batch size; color_channels refer to (R,G,B). \n",
        "In this example, we will configure our ANN to process inputs of shape (32 x 32 x 3), which is the format of CIFAR images. We can do this by passing the argument input_shape to our first layer.\n",
        "**Note** We only pass the argument input_shape to the first layer of the architecture. We do *not have to* pass it to any other layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:07:37.288653Z",
          "start_time": "2020-06-10T17:07:37.229297Z"
        },
        "id": "2Z43BeGFJM6B"
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "\n",
        "model.add(layers.Flatten(input_shape=(  32 , 32 ,  3)))  # At first we flatten the image-matrix to a vector (format needed for the fully connected layer)\n",
        "model.add(layers.Dense(1, activation=None))  \n",
        "model.add(layers.Dense(10, activation=\"softmax\"))  # output layer: there are 10 image categories (mutually exclusive); thus the layer should have 10 units; \n",
        "                                                   # activation function should be softmax that produces probabilities for the image categories\n",
        "\n",
        "# as metric we choose the accuracy: the total number of correct predictions made\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Y6hbfXJM6E"
      },
      "source": [
        "## Model details\n",
        "\n",
        "Let's look at details of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:24:41.162476Z",
          "start_time": "2020-06-10T17:24:41.157906Z"
        },
        "id": "WccMxDoOJM6E"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjKoESaaJM6H"
      },
      "source": [
        "## Training\n",
        "\n",
        "```model.fit``` trains the model.\n",
        "> * **train_images**: Training data/features\n",
        "* **train_labels**: Target\n",
        "* **epochs**: Number of times the entire dataset is fed in the model. In other words, one forward pass and one backward pass of all the training examples.\n",
        "* **batch size**: Number of images that will be fed to the network at each iteration for optimizing. In other words, the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:03.569655Z",
          "start_time": "2020-06-10T16:54:32.3542Z"
        },
        "id": "1zXaQqKqJM6H"
      },
      "source": [
        "# Training\n",
        "history = model.fit(train_images, train_labels, epochs=20, batch_size=256, \n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jDwSU-20An3"
      },
      "source": [
        "predictions = model.predict(test_images)\n",
        "predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PjQkELY1d4_"
      },
      "source": [
        "predictions_index = np.argmax(predictions, axis=1) # Convert one-hot to index; remember indexing starts from 0; index takes integers values in [0,9]\n",
        "predictions_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbZ7Y8uJM6J"
      },
      "source": [
        "## Visualize prediction\n",
        "\n",
        "Now let's visualize the prediction using the model you just trained. \n",
        "First we get the predictions with the model from the test data.\n",
        "Then we print out 15 images from the test data set, and set the titles with the prediction (and the ground truth label).\n",
        "If the prediction matches the true label, the title will be green; otherwise it's displayed in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xavvSORHJM6K"
      },
      "source": [
        "import pdb\n",
        "y_hat = model.predict(test_images)\n",
        "\n",
        "# Plot a random sample of 15 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for i, index in enumerate(np.random.choice(test_images.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(test_images[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = test_labels[index][0]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(class_names[predict_index], \n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7kohSgA6x0h"
      },
      "source": [
        "## Try/Check the following (in random order)\n",
        "\n",
        "1) Standardization vs Normalization </br>\n",
        "2) Graysclale vs RGB </br>\n",
        "3) Adding more hidden units to the network </br>\n",
        "4) Adding more layers to the network </br>\n",
        "5) Using different activation functions in the hidden units </br>\n",
        "6) Using different optimizers </br>\n",
        "7) Using different learning rates </br>\n",
        "8) Using different batch size (what's the difference in the performance and in the training time) </br>\n",
        "9) Train for more epochs </br>\n",
        "\n",
        "\n"
      ]
    }
  ]
}