{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COMP1804_Lab_8_CNN_CIFAR10_Part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97A12EGJM5n"
      },
      "source": [
        "# Convolutional Neural Network with Keras\n",
        "\n",
        "**What is Keras?** Keras is a wrapper that allows you to implement Deep Neural Networks without getting into intrinsic details of the Network. It can use *Tensorflow* or *Theano* as backend. \n",
        "\n",
        "\n",
        "In this lab you will build the *various state-of-the-art DNN networks* and explore *tranfer learning* for image classification (classify whether an image contains an airplane or automobile or bird or cat or deer or dog or frog or horse or ship or truck)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GmbVXIzJM5p"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.638667Z",
          "start_time": "2020-06-10T16:54:29.084221Z"
        },
        "id": "sZ_qSs-rJM5q"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSwAI53VJM5v"
      },
      "source": [
        "# Importing Dataset\n",
        "\n",
        "Here we are loading the cifar10 Dataset which is preloaded in tensorflow. <br>\n",
        "\n",
        "Calling the `load_data` function on this object returns splitted train and test data in form of (features, target)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.82622Z",
          "start_time": "2020-06-10T16:54:31.640652Z"
        },
        "id": "sN__Hw1sJM5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea2c6f8-c522-4dca-f71c-7dcea2e49d25"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY2BQittJM5z"
      },
      "source": [
        "# Overview of Dataset\n",
        "\n",
        "The CIFAR10 dataset contains 60,000 (32 x 32 pixel) color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.<br>\n",
        ">The shape (50000, 32, 32, 3) represents **50000** images each of dimension **32 x 32 x 3**.<br>\n",
        "The shape **(50000, )** represents (50000, 1) shape i.e. 50000 labels, each for one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.832844Z",
          "start_time": "2020-06-10T16:54:31.828686Z"
        },
        "id": "oF4wHayFJM50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541fa9f3-c8ff-4560-c81d-7b5342e946bd"
      },
      "source": [
        "print(f'Shape of the training data: {train_images.shape}')\n",
        "print(f'Shape of the training target: {train_labels.shape}')\n",
        "print(f'Shape of the test data: {test_images.shape}')\n",
        "print(f'Shape of the test target: {test_labels.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the training data: (50000, 32, 32, 3)\n",
            "Shape of the training target: (50000, 1)\n",
            "Shape of the test data: (10000, 32, 32, 3)\n",
            "Shape of the test target: (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.839396Z",
          "start_time": "2020-06-10T16:54:31.835735Z"
        },
        "id": "qfKCqRKWJM53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6302ee4-fb83-4427-fc1f-604be27c9ccf"
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6]\n",
            " [9]\n",
            " [9]\n",
            " ...\n",
            " [9]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.048741Z",
          "start_time": "2020-06-10T16:54:31.84145Z"
        },
        "id": "k8rWdBCGJM56"
      },
      "source": [
        "# To verify that the dataset looks correct, let's plot the first 16 images from the training set and display the class name below each image.\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(32,32))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m83ZM21PJM58"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "Normalizing i.e. scaling the pixels to 0-1 from 0-255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.275587Z",
          "start_time": "2020-06-10T16:54:32.05088Z"
        },
        "id": "i34Q3nNYJM59"
      },
      "source": [
        "# Normalizing\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBo6nkdi6uRY"
      },
      "source": [
        "Or alternatively standardizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSTOwMPp6vxS"
      },
      "source": [
        "#Standardizing \n",
        "import pdb\n",
        "def standardize(image_data):\n",
        "        image_data = image_data.astype(float)\n",
        "        mean = np.mean(image_data, axis=0)\n",
        "        image_data -= mean\n",
        "        std = np.std(image_data, axis=0)\n",
        "        image_data /= std\n",
        "        return image_data, mean, std\n",
        "\n",
        "train_images, mean, std =   standardize(train_images)\n",
        "\n",
        "def standardize_test(image_data, mean, std):\n",
        "        image_data = image_data.astype(float)\n",
        "        image_data -= mean\n",
        "        image_data /= std\n",
        "        return image_data\n",
        "\n",
        "test_images =   standardize_test(test_images, mean, std)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzmEthPJM6B"
      },
      "source": [
        "# Creating by yourselves the VGG16\n",
        "\n",
        "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size; color_channels refer to (R,G,B). \n",
        "In this example, we will build the VGG16 network and configure it to process inputs of shape (32, 32, 3). We can do this by passing the argument input_shape to our first layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:07:37.288653Z",
          "start_time": "2020-06-10T17:07:37.229297Z"
        },
        "id": "2Z43BeGFJM6B"
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
        "model.add(layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))  ## END BLOCK 1\n",
        "model.add(layers.Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))   ## END BLOCK 2\n",
        "model.add(layers.Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))    ## END BLOCK 3 \n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))    ## END BLOCK 4 \n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))    ## END BLOCK 5 \n",
        "model.add(layers.Flatten())    ## converting to vector \n",
        "model.add(layers.Dense(4096, activation=\"relu\"))    ## 1st FC layer \n",
        "model.add(layers.Dense(4096, activation=\"relu\"))    ## 2st FC layer \n",
        "model.add(layers.Dense(10, activation=\"softmax\"))    ## output layer \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPnvL5iV4rvK"
      },
      "source": [
        "# Using the built-in Keras code and weights of VGG16\n",
        "\n",
        "\n",
        "Keras provides access to a number of top-performing pre-trained models that were developed for image recognition tasks.\n",
        "\n",
        "They are available via the Applications API, and include functions to load a model with or without the pre-trained weights, and prepare data in a way that a given model may expect (e.g. scaling of size and pixel values).\n",
        "\n",
        "The first time a pre-trained model is loaded, Keras will download the required model weights, which may take some time given the speed of your internet connection. \n",
        "\n",
        "When loading a given model, the “include_top” argument can be set to False, in which case the model's fully-connected layers and the output layer, will not be loaded, allowing new layers to be added and trained. A model without a top will output activations from the last convolutional or pooling layer directly.\n",
        "Additionally, when the “include_top” argument is False, the “input_tensor” argument must be specified, allowing the expected fixed-sized input of the model to be changed. \n",
        "\n",
        "Alternately, we may wish to use the VGG16 model layers, but train the new layers of the model without updating the weights of the VGG16 layers (aka freeze these weights). This will allow the new added layers to learn to interpret the learned features of the VGG16.\n",
        "This can be achieved by setting the “trainable” property on each of the layers in the loaded VGG model to False prior to training. \n",
        "You can pick and choose which layers are trainable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY2zBnsKu0bl"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "\n",
        "# load model without classifier layers\n",
        "model = VGG16(include_top=False, weights=\"imagenet\", input_shape=(32, 32, 3)) \n",
        "    \n",
        "### If we want to freeze these pretrained weights:\n",
        "\n",
        "#for layer in model.layers:\n",
        "#\tlayer.trainable = False     # mark loaded layers as not trainable\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = layers.Flatten()(model.layers[-1].output)\n",
        "fc1 = layers.Dense(4096, activation='relu')(flat1)\n",
        "fc2 = layers.Dense(4096, activation='relu')(fc1)\n",
        "output = layers.Dense(10, activation='softmax')(fc2)\n",
        "\n",
        "\n",
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjKoESaaJM6H"
      },
      "source": [
        "## Training\n",
        "\n",
        "```model.fit``` trains the model.\n",
        "> * **train_images**: Training data/features\n",
        "* **train_labels**: Target\n",
        "* **epochs**: Number of times the entire dataset is fed in the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:03.569655Z",
          "start_time": "2020-06-10T16:54:32.3542Z"
        },
        "id": "1zXaQqKqJM6H"
      },
      "source": [
        "# Training\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=512,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbZ7Y8uJM6J"
      },
      "source": [
        "## Visualize prediction\n",
        "\n",
        "Now let's visualize the prediction using the model you just trained. \n",
        "First we get the predictions with the model from the test data.\n",
        "Then we print out 15 images from the test data set, and set the titles with the prediction (and the ground truth label).\n",
        "If the prediction matches the true label, the title will be green; otherwise it's displayed in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xavvSORHJM6K"
      },
      "source": [
        "import pdb\n",
        "y_hat = model.predict(test_images)\n",
        "\n",
        "# Plot a random sample of 15 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for i, index in enumerate(np.random.choice(test_images.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(test_images[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = test_labels[index][0]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(class_names[predict_index], \n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0nQk6ukS4OF"
      },
      "source": [
        "# Using the built-in Keras code and weights of ResNet\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2s2r5WUS4OW"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "\n",
        "# load model without classifier layers\n",
        "model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(32, 32, 3)) \n",
        "    \n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = layers.Flatten()(model.layers[-1].output)\n",
        "output = layers.Dense(10, activation='softmax')(flat1)\n",
        "\n",
        "\n",
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXot1qt7S4OX"
      },
      "source": [
        "## Model details\n",
        "\n",
        "Let's look at details of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:24:41.162476Z",
          "start_time": "2020-06-10T17:24:41.157906Z"
        },
        "id": "cGKsvUH_S4OX"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3RDEVTLS4OX"
      },
      "source": [
        "## Training\n",
        "\n",
        "```model.fit``` trains the model.\n",
        "> * **train_images**: Training data/features\n",
        "* **train_labels**: Target\n",
        "* **epochs**: Number of times the entire dataset is fed in the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:03.569655Z",
          "start_time": "2020-06-10T16:54:32.3542Z"
        },
        "id": "fEKnMcsFS4OY"
      },
      "source": [
        "# Training\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=256,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja918I0pS4OY"
      },
      "source": [
        "## Visualize prediction\n",
        "\n",
        "Now let's visualize the prediction using the model you just trained. \n",
        "First we get the predictions with the model from the test data.\n",
        "Then we print out 15 images from the test data set, and set the titles with the prediction (and the ground truth label).\n",
        "If the prediction matches the true label, the title will be green; otherwise it's displayed in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN2DsDVxS4OY"
      },
      "source": [
        "import pdb\n",
        "y_hat = model.predict(test_images)\n",
        "\n",
        "# Plot a random sample of 15 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for i, index in enumerate(np.random.choice(test_images.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(test_images[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = test_labels[index][0]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(class_names[predict_index], \n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8USRJNcM7WYO"
      },
      "source": [
        "# Using the built-in Keras code and weights of other state-of-the-art DNNs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhqbr9rD8NY6"
      },
      "source": [
        "For potential models to use have a  look at (you can also find their performance on ImageNet):\n",
        "https://keras.io/api/applications/\n",
        "\n",
        "Generally read the documentation when you are using a model as this explains the arguments needed and also the input format needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF4h9lGA8y8G"
      },
      "source": [
        "from keras.applications import ResNet101, ResNet152,  ResNet50V2, ResNet101V2, ResNet152V2, DenseNet121, DenseNet169, DenseNet201, NASNetLarge\n",
        "from keras.models import Model\n",
        "\n",
        "# for EfficientNet have a look here: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/image_classification_efficientnet_fine_tuning.ipynb\n",
        "# have a careful look at the documentation as well, for instance Keras built-in EfficientNet doesn't want normalised input as it is normalising it by itself (the code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdADabHK7WYa"
      },
      "source": [
        "# load model without classifier layers\n",
        "model = ResNet101(include_top=False, weights=\"imagenet\", input_shape=(32, 32, 3)) # or you can load here any other network\n",
        "    \n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = layers.Flatten()(model.layers[-1].output)\n",
        "output = layers.Dense(10, activation='softmax')(flat1)\n",
        "\n",
        "\n",
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsBxhKOG7WYb"
      },
      "source": [
        "## Model details\n",
        "\n",
        "Let's look at details of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:24:41.162476Z",
          "start_time": "2020-06-10T17:24:41.157906Z"
        },
        "id": "kJrOZ9_D7WYb"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8tnqLYb7WYb"
      },
      "source": [
        "## Training\n",
        "\n",
        "```model.fit``` trains the model.\n",
        "> * **train_images**: Training data/features\n",
        "* **train_labels**: Target\n",
        "* **epochs**: Number of times the entire dataset is fed in the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:03.569655Z",
          "start_time": "2020-06-10T16:54:32.3542Z"
        },
        "id": "TR-r7yES7WYb"
      },
      "source": [
        "# Training\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=256,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFH5bBjH7WYc"
      },
      "source": [
        "## Visualize prediction\n",
        "\n",
        "Now let's visualize the prediction using the model you just trained. \n",
        "First we get the predictions with the model from the test data.\n",
        "Then we print out 15 images from the test data set, and set the titles with the prediction (and the ground truth label).\n",
        "If the prediction matches the true label, the title will be green; otherwise it's displayed in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLqHE6W47WYc"
      },
      "source": [
        "import pdb\n",
        "y_hat = model.predict(test_images)\n",
        "\n",
        "# Plot a random sample of 15 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for i, index in enumerate(np.random.choice(test_images.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(test_images[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = test_labels[index][0]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(class_names[predict_index], \n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8BZCkYBJM6P"
      },
      "source": [
        "# Visualising feature maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:05.523866Z",
          "start_time": "2020-06-10T16:59:03.572384Z"
        },
        "id": "aKprwqLxJM6P"
      },
      "source": [
        "from numpy import expand_dims\n",
        "from keras.models import Model\n",
        "\n",
        "# redefine model to output right after the first conv layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[1].output)   ### here you need to select only conv or pooling layers; the index should always start from 1; index 0 corresponds to the input layer\n",
        "model1.summary()\n",
        "# load the image with the required shape\n",
        "img = test_images[0]   ### if you want change the index here to load another image from the test/training set\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# get feature map for first conv layer\n",
        "feature_maps = model1.predict(img)\n",
        "# plot all 64 maps in 8x8 squares   \n",
        "### square1 times square2 should always be the number of output feature maps of the layer \n",
        "### (in other words should be the last number of the 'output shape' column of the model summary)\n",
        "square1 = 8\n",
        "square2 = 8\n",
        "ix = 1\n",
        "\n",
        "# Display the input image\n",
        "plt.imshow(img[0])\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for _ in range(square1):\n",
        "\tfor _ in range(square2):\n",
        "\t\tax = figure.add_subplot(square1, square2, ix, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tax.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZXRp3T1RNSx"
      },
      "source": [
        "## Try/Check the following (in random order)\n",
        "\n",
        "\n",
        "1) Run the VGG16 from scratch, i.e., from the 'Creating by yourselves the VGG16' Section. Try using different learning rates (e.g: 0.001 and 0.0001) </br>\n",
        "2) Use transfer learning on the pre-trained VGG16, i.e., run the code on  'Using the built-in Keras code and weights of VGG16' Section. Try using different learning rates (e.g: 0.001 and 0.0001)  </br>\n",
        "3) Use transfer learning on the pre-trained VGG16 and keep its conv layers freeze (meaning not train these layers and train only the fully connected ones), i.e., run the code on 'Using the built-in Keras code and weights of VGG16' Section and uncomment the commented code that freezes the weights. Try using different learning rates (e.g: 0.001 and 0.0001) </br>\n",
        "\n",
        "Also try changing the number of units in the fully connected layers. </br>\n",
        "Also try using different batch sizes. </br>\n",
        "Allso try training for more epochs.</br>\n",
        "\n",
        "Compare performance in all cases. </br>\n",
        "\n",
        "Finally check feature maps in these cases (try to check feature maps of different layers and of different test images) </br>\n",
        "\n",
        "\n",
        "Try similar things as above with the other state-of-the-art networks. Try to add more fully connected layers on top of them (and before the output layer). </br>\n",
        "Try to not load their pretrained weights but train them from scratch:\n",
        "in the code replace:   weights=\"imagenet\"   ->     weights=None   </br>\n",
        "\n",
        "Compare performance between VGG16 and the other networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuq4vCo4b0Tj"
      },
      "source": [
        "# Data Augmentation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vA4HmwgakoL"
      },
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from keras.models import Model\n",
        "from tensorflow import keras\n",
        "\n",
        "##  RandomContrast: Adjust the contrast of an image or images by a random factor.\n",
        "##  RandomCrop: Randomly crop the images to target height and width.\n",
        "##  RandomFlip: Randomly flip each image horizontally and vertically.\n",
        "##  RandomHeight: Randomly vary the height of a batch of images during training.\n",
        "##  RandomRotation: Randomly rotate each image.\n",
        "##  RandomTranslation: Randomly translate each image during training.\n",
        "##  RandomWidth: Randomly vary the width of a batch of images during training.\n",
        "##  RandomZoom: Randomly zoom each image during training.\n",
        "##  Rescaling: Multiply inputs by scale and adds offset.\n",
        "##  Resizing: Image resizing layer.\n",
        "\n",
        "# for specific details on the preprocessing layers have a look here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing\n",
        "data_augmentation = keras.Sequential(\n",
        "    [   #preprocessing.Resizing(height=32, width=32, interpolation=\"bilinear\"),\n",
        "        preprocessing.RandomFlip(\"horizontal\"),\n",
        "        preprocessing.RandomRotation(0.1),\n",
        "        preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "# the augmentation is being made part of the model. Note that data augmentation is inactive at test time.\n",
        "\n",
        "# Create a model that includes the augmentation stage\n",
        "input_shape = (32, 32, 3)\n",
        "classes = 10\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "# Augment images\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "# Add the rest of the model\n",
        "model = keras.applications.ResNet50(include_top=False,\n",
        "    weights='imagenet', input_shape=input_shape)(x)\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = layers.Flatten()(model)\n",
        "outputs = layers.Dense(10, activation='softmax')(flat1)\n",
        "\n",
        "\n",
        "\n",
        "# define new model\n",
        "model = Model(inputs=inputs, outputs= outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# initialize the number of epochs and batch size\n",
        "EPOCHS = 10\n",
        "BS = 256\n",
        "\n",
        "\n",
        "# train the network\n",
        "history = model.fit(train_images, train_labels, shuffle= True,\n",
        "\tvalidation_data=(test_images, test_labels), steps_per_epoch=len(train_images) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOlO71ttTaNN"
      },
      "source": [
        "</br></br>\n",
        "\n",
        "</br></br>\n",
        "</br></br>\n",
        "# Helpful for CW \n",
        "\n",
        "</br></br></br></br></br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4p-tJnqRcPu"
      },
      "source": [
        "# Upload image in Colab, Load image, Resize image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjHEtCorOCGo"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXrIEKpbOpEA"
      },
      "source": [
        "import cv2,glob\n",
        "\n",
        "items = glob.glob('/content/images/*')\n",
        "aaa = []\n",
        "for each_image in items:\n",
        "  #if each_image.endswith(\".jpg\") or each_image.endswith(\".jpeg\"):\n",
        "\n",
        "   image = cv2.imread(each_image)\n",
        "   image = cv2.resize(image, (96,96))\n",
        "   aaa.append(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4hbTZiNTNn-"
      },
      "source": [
        "# Save model during training, compute evaluation metrics\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0pGAp72UBqJ"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for specific details on the preprocessing layers have a look here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing\n",
        "data_augmentation = keras.Sequential(\n",
        "    [   #preprocessing.Resizing(height=32, width=32, interpolation=\"bilinear\"),\n",
        "        preprocessing.RandomFlip(\"horizontal\"),\n",
        "        preprocessing.RandomRotation(0.1),\n",
        "        preprocessing.RandomZoom(0.1),\n",
        "    ]\n",
        ")\n",
        "# the augmentation is being made part of the model. Note that data augmentation is inactive at test time.\n",
        "\n",
        "# Create a model that includes the augmentation stage\n",
        "input_shape = (32, 32, 3)\n",
        "classes = 10\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "# Augment images\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "# Add the rest of the model\n",
        "model = keras.applications.ResNet50(include_top=False,\n",
        "    weights='imagenet', input_shape=input_shape)(x)\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = layers.Flatten()(model)\n",
        "outputs = layers.Dense(10, activation='softmax')(flat1)\n",
        "\n",
        "\n",
        "\n",
        "# define new model\n",
        "model = Model(inputs=inputs, outputs= outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define the checkpoint, have a look here for the arguments: https://keras.io/api/callbacks/model_checkpoint/\n",
        "filepath = \"ResNet-{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
        "checkpoint = ModelCheckpoint(  \n",
        "    filepath,\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=1,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    mode=\"auto\",\n",
        "    save_freq=\"epoch\")\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# initialize the number of epochs and batch size\n",
        "EPOCHS = 2\n",
        "BS = 256\n",
        "\n",
        "\n",
        "# train the network\n",
        "history = model.fit(train_images, train_labels, shuffle= True,\n",
        "\tvalidation_data=(test_images, test_labels), steps_per_epoch=len(train_images) // BS,\n",
        "\tepochs=EPOCHS, callbacks=callbacks_list)\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHb4JfFIYC6j"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# here you need to specify the saved model from before\n",
        "filepath = \"/content/ResNet-02-0.1784.h5\"\n",
        "\n",
        "# load the model\n",
        "new_model = load_model(filepath)\n",
        "\n",
        "# for the metrics and details have a look here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "y_pred1 = new_model.predict(test_images)\n",
        "y_pred = np.argmax(y_pred1, axis=1)\n",
        "\n",
        "\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "\n",
        "# Print f1, precision, and recall scores\n",
        "print(precision_score(test_labels, y_pred , average=\"macro\"))\n",
        "print(recall_score(test_labels, y_pred , average=\"macro\"))\n",
        "\n",
        "print(f1_score(test_labels, y_pred , average=\"macro\"))\n",
        "print(f1_score(test_labels, y_pred , average=\"micro\"))\n",
        "print(f1_score(test_labels, y_pred , average=\"weighted\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}