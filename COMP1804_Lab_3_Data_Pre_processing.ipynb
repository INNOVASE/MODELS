{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP1804_Lab_3_Data_Pre_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8UngIdVhz8C0",
        "1L5pOUpk20lG",
        "y89foQNY3BOM",
        "876pgyH_jRUc",
        "g2fKhoAym27r",
        "DTxyo3F3mIjJ",
        "HNBCYMPG2ZQs"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN_Z8Y5a2N3R"
      },
      "source": [
        "___\n",
        "# COMP1804 Lab 3 - Data Pre-processing\n",
        "\n",
        "\n",
        "\n",
        "**Learning Objectives:**\n",
        " *  Understand the stages of data pre-processing.\n",
        " *  Use Python for pre-processing. \n",
        " *  Practice the different stages of data pre-processing in Python.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM1iaD-ka3Y1"
      },
      "source": [
        "## 1. Loading the Dataset\n",
        "\n",
        "\n",
        "For this Lab, I have used a subset of the Loan Prediction dataset. You can download the training and testing dataset from here: [Download Data](https://drive.google.com/drive/folders/10beInU2j3J9tF_dpbt50_36oxoa9jJQx?usp=sharing)\n",
        "\n",
        "Note : Testing data that you are provided is the subset of the training data from Loan Prediction problem.\n",
        "\n",
        " \n",
        "\n",
        "Now, lets get started by importing important packages and the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQzIVnbnmWGM"
      },
      "source": [
        "**1.1 Import the necessary Python modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko6pLK6JmkYP"
      },
      "source": [
        "# Load python modules\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5eEPJekd8QE"
      },
      "source": [
        "**1.2 Load Dataset **\n",
        "\n",
        "Note: Download the csv files from the URL to your local drive and load from there as shown in the code below.\n",
        "\n",
        "We are using pandas to load the data. We will also use pandas next to explore the data both with descriptive statistics and data visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRPwy7LQay_"
      },
      "source": [
        "# Load dataset from local drive (for colab notebook)\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()    # Will prompt you to select file\n",
        "train_dataset = pd.read_csv(io.BytesIO(uploaded['X_train.csv']))\n",
        "\n",
        "uploaded = files.upload()    # Will prompt you to select file\n",
        "train_labels = pd.read_csv(io.BytesIO(uploaded['Y_train.csv']))\n",
        "\n",
        "uploaded = files.upload()    # Will prompt you to select file\n",
        "test_dataset = pd.read_csv(io.BytesIO(uploaded['X_test.csv']))\n",
        "\n",
        "uploaded = files.upload()    # Will prompt you to select file\n",
        "test_labels = pd.read_csv(io.BytesIO(uploaded['Y_test.csv']))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeYYLoV9b9fB"
      },
      "source": [
        "**1.2.1 Inspect Dataset **\n",
        "\n",
        "**1.2.1.1 Dimensions of Dataset **\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lyOFY3_P0-Y"
      },
      "source": [
        "# shape\n",
        "print(train_dataset.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nh_Wj7-Keel"
      },
      "source": [
        "print(test_dataset.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fmOZAI8nrQm"
      },
      "source": [
        "# list of column titles \n",
        "print(train_dataset.columns)\n",
        "print(train_labels.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkN5TfTNoCJ1"
      },
      "source": [
        "# list of column (field) data types\n",
        "print(train_dataset.dtypes)\n",
        "print(train_labels.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxkbyWHMd5aQ"
      },
      "source": [
        "**1.2.1.2 Take a peek at the Dataset **\n",
        "\n",
        "Note that \"NaN\" means \"Not a Number\". It is not the same as 0. Python replaces empty fields with \"NaN\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k68kOMPXeIK5"
      },
      "source": [
        "# head\n",
        "train_dataset.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-xAOJeMiXFB"
      },
      "source": [
        "##2. Managing Missing Data\n",
        "\n",
        "Sometimes you may find some data are missing in the dataset. If the missing values are not handled properly inaccurate inference about the data may result. Due to improper handling, the result obtained will differ from ones where the missing values are present. \n",
        "\n",
        "Since missing values can tangibly reduce prediction accuracy, this step needs to be a priority. In terms of machine learning, assumed or approximated values are “more appropriate” for an algorithm than just missing ones.  Even if you don’t know the exact value, methods exist to better “assume” which value is missing or bypass the issue. So how to сlean the data here? Choosing the right approach also heavily depends on data and the domain you have:\n",
        "* Substitute missing values with dummy values, e.g. n/a for categorical or 0 for numerical values.\n",
        "* Substitute the missing numerical values with mean figures.\n",
        "* For categorical values, you can also use the most frequent items to fill in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpp_-krCmBZn"
      },
      "source": [
        "### 2.1 Removing Missing Data\n",
        "\n",
        "So how can we handle missing data. One obvious idea is to remove the lines (observations recorded in the rows) where there is some missing data. That is ok if you’ve got large datasets. The dataset we have here is considerably small, so removing data will have crucial impact. We will therefore need to look for alternative methods to deal with the missing data. Removing removing rows can be quite dangerous as you will be deleting crucial information.\n",
        "\n",
        "The decission to remove data will depend on the size of dataset and the problem domain (type of data collected).\n",
        "\n",
        "Assuming it was appropriate to remove observation rows, the following code will help. \n",
        "Given the original dataset in figure 2, we could remove rows 4 and 6 as they containg missing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gUPCuUlcm4K"
      },
      "source": [
        "**2.1.1 Remove all rows that contain missing data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPq8zEp5zRNk"
      },
      "source": [
        "# remove all rows with missing data\n",
        "print(train_dataset.dropna())\n",
        "train_dataset.head(20)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K7cvvJntFW-"
      },
      "source": [
        "**2.1.2 Remove specific rows**\n",
        "\n",
        "Drop specific rows by passing index labels to the drop method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aSXQ-NpziR_"
      },
      "source": [
        "# remove selected rows\n",
        "\n",
        "print(train_dataset.drop([0]))\n",
        "train_dataset.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sbP6dBjuzPP"
      },
      "source": [
        "**2.1.3 Remove specific columns**\n",
        "\n",
        "Usually you would drop particular columns especially if all or most of its values are missing. The drop method can also be used here with parameters to define the column title and axis=1 to denote that we want to drop a column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gPNFylQiZef"
      },
      "source": [
        "# remove selected column\n",
        "\n",
        "print(train_dataset.drop(\"Gender\", axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq5Ov8dLjZba"
      },
      "source": [
        "# remove multiple selected columns\n",
        "\n",
        "print(train_dataset.drop([\"Gender\", \"Married\"], axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKNXZWt_j1LA"
      },
      "source": [
        "** 2.1.4 Remove all rows where data satisfies a condition in a particular column**\n",
        "\n",
        "Drop all rows where Gender is Female (and NaN). So filter the dataframe where Gender is Male.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qSQds1DkBP1"
      },
      "source": [
        "# remove all rows where Gender is Female (and NaN).\n",
        "\n",
        "print(train_dataset[train_dataset[\"Gender\"] == \"Male\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu_p41cLMFvT"
      },
      "source": [
        "** 2.1.4 Remove all rows where data satisfies a condition in a particular column**\n",
        "\n",
        "Drop all rows where Gender is not Male. So keep rows where Gender is Female or NaN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a4E5HNeMDyn"
      },
      "source": [
        "# remove all rows where Gender is Female (and NaN).\n",
        "\n",
        "print(train_dataset[train_dataset[\"Gender\"] != \"Male\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEiJQTbUnR6w"
      },
      "source": [
        "**2.1.5 Remove all rows where data is missing in a particular column**\n",
        "\n",
        "Remove all rows where a particular column has a missing value. This will result in a dataset that has no missing values in that particular column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GNHo6pbnspV"
      },
      "source": [
        "# remove all rows where ApplicantIncome is missing.\n",
        "# 1. check to see if there are missing data in the ApplicantIncome column\n",
        "print(train_dataset[\"ApplicantIncome\"].isnull().sum())\n",
        "\n",
        "print(train_dataset.head(20))\n",
        "\n",
        "\n",
        "# 2. drop all rows where a value for ApplicantIncome is missing\n",
        "print(train_dataset[train_dataset[\"ApplicantIncome\"].notnull()])\n",
        "\n",
        "# the first row contains a null and is removed\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p92NjXKYqljj"
      },
      "source": [
        "### 2.2 Filling in Missing Data\n",
        "\n",
        "As alternative to removing rows with missing data, you can estimate plausible values for the missing data instead. For example, replace the missing data within a column with a value equivalent to the mean of all the values in that column. That can also be applied to every feature where there is missing data.\n",
        "\n",
        "\n",
        "We will use the scikit-learn library. The impute class allows us to manipulate the missing data.\n",
        "\n",
        "\n",
        "At first we will create an object of the imputer class. \n",
        "The class has an argument called 'strategy' which shows what method to use to generate a value replacement for the missing value. 'mean' is the default value for strategy.\n",
        "\n",
        "If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
        "\n",
        "If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
        "\n",
        "If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
        "\n",
        "If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE5LaYCyTPA4"
      },
      "source": [
        "print(train_dataset[\"Gender\"].isnull().sum())\n",
        "\n",
        "print(train_dataset[\"ApplicantIncome\"].isnull().sum())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALKlOLLnre0C"
      },
      "source": [
        "# handling missing data\n",
        "from sklearn.impute import SimpleImputer \n",
        "\n",
        "train_dataset_no_nans =  train_dataset.copy()\n",
        "\n",
        "# 1. Imputer\n",
        "imptr = SimpleImputer(missing_values = np.nan, strategy = 'mean')  \n",
        "\n",
        "\n",
        "# 2. Fit the imputer object to the feature matrix (only for numeric features)\n",
        "imptr = imptr.fit(train_dataset_no_nans[['ApplicantIncome', 'CoapplicantIncome',\n",
        "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
        "\n",
        "# 3. Call Transform to replace missing data in train_dataset (on specific columns) by the mean of the column to which that missing data belongs to\n",
        "train_dataset_no_nans[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']] = \\\n",
        "imptr.transform(train_dataset_no_nans[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', 'Loan_Amount_Term', 'Credit_History']]) \n",
        "\n",
        "train_dataset_no_nans\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6noezOH24i9H"
      },
      "source": [
        "# 1. Imputer\n",
        "imptr = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')  \n",
        "\n",
        "# 2. Fit the imputer object to the feature matrix (only for numeric features)\n",
        "imptr = imptr.fit(train_dataset[['Gender']])\n",
        "\n",
        "# 3. Call Transform to replace missing data in train_dataset (on specific columns) by the mean of the column to which that missing data belongs to\n",
        "train_dataset_no_nans[['Gender']] = imptr.transform(train_dataset[['Gender']]) \n",
        "\n"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_EqaXD4y3mB"
      },
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IjFUE5ftSP3"
      },
      "source": [
        "##  Exercise no 1\n",
        "\n",
        "Fill in the missing values for the categorical features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-oBDfMtyjyi"
      },
      "source": [
        "### insert code here; work on the: train_dataset_no_nans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roNmuk_Xynbp"
      },
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UngIdVhz8C0"
      },
      "source": [
        "##3.  Categorical Data\n",
        "\n",
        "Data Preprocessing in machine learning requires values of the data in numerical form. Therefore text values in the columns of datasets must be converted into numerical form. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L5pOUpk20lG"
      },
      "source": [
        "###3.1 Converting categorical to numerical values\n",
        "\n",
        "Given the original dataset, it is clear we have a few categorical features. All these need to be encoded. The [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class is used to transform the categorical or string values to numerical ones (between 0 and n_classes-1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN55GrDX0jzO"
      },
      "source": [
        "dummy = train_dataset_no_nans.copy()\n",
        "\n",
        "# encode categorical data for the 'Gender' column\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lblEncoder_X = LabelEncoder()                 # create an object of the LabelEncoder class\n",
        "dummy['Gender'] = lblEncoder_X.fit_transform(dummy['Gender']) # apply LblEncoder object to our categorical variables (columns - 'Gender') using the fit_transform method. This returns the column encoded.\n",
        "\n",
        "print(dummy)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d67vCZRd1dXp"
      },
      "source": [
        "Now let us try a different encoding strategy, the one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49B60ZcAMa4X"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "oneHotEncoder = OneHotEncoder()                                      # create OneHotEncoder object \n",
        "aaa = oneHotEncoder.fit(train_dataset_no_nans[['Gender']])           # fit the OneHotEncoder object to feature Gender\n",
        "aaa.categories_\n",
        "\n",
        "dum = aaa.transform(train_dataset_no_nans[['Gender']]).toarray()               # dum is an array of shape (391,2) containing the one-hot encoding of the feature Gender of the dataframe train_dataset_no_nans\n",
        "dum\n",
        "\n",
        "train_dataset_no_nans['Female'] = dum[:,0]     # we add to the train_dataset_no_nans one feature column called Female and add there the corresponding encoded values\n",
        "train_dataset_no_nans['Male'] = dum[:,1]       # we add to the train_dataset_no_nans one feature column called Male and add there the corresponding encoded values\n",
        "train_dataset_no_nans= train_dataset_no_nans.drop(columns='Gender',axis=1)  # we delete the (former) column/feature Gender\n",
        "\n",
        "train_dataset_no_nans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSlYx59cCWFa"
      },
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N11ccJXCcRf"
      },
      "source": [
        "##  Exercise no 2\n",
        "\n",
        "Perform label encoding and one-hot encoding for all categorical features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TulpHo7pCYx1"
      },
      "source": [
        "# place your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP7Yit9oCvra"
      },
      "source": [
        "##  Exercise no 3\n",
        "\n",
        "Perform label encoding for the labels (i.e., the train_labels **dataframe**)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euLz0XZtCu8u"
      },
      "source": [
        "# place your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU0CiygLDAD8"
      },
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y89foQNY3BOM"
      },
      "source": [
        "##4.  Compute Statistics and Check Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XlOOXLmDbYo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# compute histograms of some numeric features (you can see the ranges)\n",
        "train_dataset_no_nans[train_dataset_no_nans.dtypes[(train_dataset_no_nans.dtypes==\"float64\") | (train_dataset_no_nans.dtypes==\"int64\")].index.values].hist(figsize=[15,15])  # the syymbol '|' means 'or'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J6YlpOBLIYM"
      },
      "source": [
        "sampleClassBias = train_labels['Target'].value_counts()\n",
        "print('Training Labels distribution:')\n",
        "print(sampleClassBias)\n",
        "\n",
        "sampleClassBias = test_labels['Target'].value_counts()\n",
        "print('Test Labels distribution:')\n",
        "print(sampleClassBias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "876pgyH_jRUc"
      },
      "source": [
        "##5.  Feature Scaling\n",
        "\n",
        "When the data is comprised of feature values with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTxyo3F3mIjJ"
      },
      "source": [
        "###5.1 Implementing Feature Scaling\n",
        "\n",
        "Feature scaling is a method used to scale the range of variables/values of features. \n",
        "\n",
        ">![feature scaling](https://drive.google.com/uc?id=1loaMbVo_7ZJo53Ogv7I0XMG_wSOOYBxI)\n",
        "\n",
        ">  Figure 5: Feature Scaling methods\n",
        "> Where X is the observation feature.\n",
        "\n",
        "There are several ways of scaling the data. One way is called **Standardisation** which may be used. For every observation of the selected column, our program will apply the formula of standardisation and fit it to a scale. That is for each observation and each feature within the mean value is withdrawn (subtracted ) from all the values of the feature and divide by the standard deviation. \n",
        "The other common type of scaling is **normalisation** where the minimal value of all the feature values is subtracted from the observation feature X and divided by the difference between the max of the feature values and the min of the feature values.\n",
        "\n",
        "**It is not essential to understand the math behind these methods, what is important is to remember that the variables are being put in the same range / same scale so that no variable is dominated by another.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh3kGziemfHW"
      },
      "source": [
        "#normalisation\n",
        "ddummy = train_dataset_no_nans.copy()\n",
        "\n",
        "\n",
        "# Importing MinMaxScaler and initializing it\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "min_max=MinMaxScaler()\n",
        "# Normalising\n",
        "train_dataset_no_nans[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', 'Loan_Amount_Term', 'Credit_History']]=min_max.fit_transform(train_dataset_no_nans[['ApplicantIncome', 'CoapplicantIncome',\n",
        "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
        "\n",
        "train_dataset_no_nans"
      ],
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Bal0MozwaU"
      },
      "source": [
        "\n",
        "Question: Do we need to apply feature scaling to the labels? The answer is no! The label is a categorical value that takes 2 values either no (0) or yes (1).  It is a classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBWg6tpxOhhX"
      },
      "source": [
        "# Standardisation\n",
        "train_dataset_no_nans1 = ddummy.copy()\n",
        "\n",
        "#Standardizing \n",
        "from sklearn.preprocessing import scale\n",
        "train_dataset_no_nans1[['ApplicantIncome', 'CoapplicantIncome','LoanAmount', 'Loan_Amount_Term', 'Credit_History']]=scale(train_dataset_no_nans1[['ApplicantIncome', 'CoapplicantIncome',\n",
        "                'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
        "train_dataset_no_nans1"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoEPeJG1POmL"
      },
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcTnN7tCOOJe"
      },
      "source": [
        "##  Exercise no 4\n",
        "\n",
        "Perform normalisation on the other features too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVh61XEXQA7M"
      },
      "source": [
        "# place code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfR46WmGPUSW"
      },
      "source": [
        "##  Exercise no 5\n",
        "\n",
        "Perform standardisation on the other features too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLErRHNLP-1O"
      },
      "source": [
        "# place code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83kaz7TjP91z"
      },
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNBCYMPG2ZQs"
      },
      "source": [
        "##7. Final\n",
        "\n",
        "The steps below can be used as a base for ML projects with small variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t6wx3GBPxAL"
      },
      "source": [
        "##  Exercise no 6\n",
        "\n",
        "Perform all the previous data pre-processing steps to the test data and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h8vleH_QHtR"
      },
      "source": [
        "# place code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxTzOoCuUZkl"
      },
      "source": [
        "Next I provide a standard classifier so that you perform some tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z89VUxLP2kFd"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# we will use a classifier (we will skip details and what it does for now, so just use it as it is)\n",
        "knn=KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_dataset_no_nans1, train_labels['Target'])  \n",
        "## we train the classifier with the training data and labels: train_dataset_no_nans1 should be the training dataframe after:  \n",
        "## i) filling in all missing values, ii) encoding all categorical features and (maybe) after feature scaling\n",
        "\n",
        "# Checking the model's accuracy (performance); this should be performed on the test set and thus we use the test_dataset_no_nans1 (same pre-processing as before shoud have been performed) and the test labels (after encoding)\n",
        "accuracy_score(test_labels['Target'],knn.predict(test_dataset_no_nans1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQWjGIFRTlyt"
      },
      "source": [
        "##  Final Exercise \n",
        "\n",
        "Test the performance of the classifier when:\n",
        "\n",
        "- the label encoder has been used and no feature scaling has been performed\n",
        "- the label encoder has been used and normalisation has been performed \n",
        "- the label encoder has been used and standardisation has been performed \n",
        "- the one-hot encoder has been used and no feature scaling has been performed\n",
        "- the one-hot encoder has been used and normalisation has been performed \n",
        "- the one-hot encoder has been used and standardisation has been performed \n",
        "\n",
        "Maybe the performance can be further improved? Have a look at the features that you used and think whether data quality assessment can be performed."
      ]
    }
  ]
}